{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1: due week 5 just before class at 1759 hours\n",
    "Given a fully connected Neural Network as follows:\n",
    "1. Input (x1,x2): 2 nodes\n",
    "\n",
    "2. First hidden layer: 10 nodes, with weights (w) and bias (b), sigmoid activation function\n",
    "\n",
    "3. Second hidden layer: 10 nodes, with weights (w) and bias (b), sigmoid activation function\n",
    "\n",
    "4. Output (predict): 1 node\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Implement this neural network in pytorch\n",
    "\n",
    "(2) Generate the input data (x1,x2) \\in [0,1] drawn from a uniform random distribution\n",
    "\n",
    "(3) Generate the labels y = (x1*x1+x2*x2)/2\n",
    "\n",
    "(4) Implement a loss function L = (predict-y)^2\n",
    "\n",
    "(5) Use batch size of 1, that means feed data one point at a time into network and compute the loss. Do one time forward propagation with one data point.\n",
    "\n",
    "(6) Compute the gradients using pytorch autograd:\n",
    "\n",
    "a. dL/dw, dL/db\n",
    "\n",
    "b. Print these values into a text file: torch_autograd.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X:  tensor([[0.4963, 0.7682]])\n",
      "Input y:  tensor([[0.4182]])\n",
      "step 1: loss=0.986679   \n",
      "First hidden layer weights gradient: \n",
      " tensor([[-0.0028,  0.0071, -0.0034, -0.0120, -0.0009, -0.0063, -0.0078, -0.0040,\n",
      "          0.0051,  0.0050],\n",
      "        [-0.0043,  0.0110, -0.0053, -0.0185, -0.0015, -0.0097, -0.0121, -0.0062,\n",
      "          0.0080,  0.0078]])\n",
      "First hidden layer bias gradient: \n",
      " tensor([-0.0056,  0.0143, -0.0069, -0.0241, -0.0019, -0.0126, -0.0158, -0.0081,\n",
      "         0.0104,  0.0102])\n",
      "Second hidden layer weight gradient:\n",
      " tensor([[-0.0402,  0.0034,  0.0420,  0.0224, -0.0303,  0.0003,  0.0236,  0.0363,\n",
      "          0.0421,  0.0394],\n",
      "        [-0.0839,  0.0072,  0.0875,  0.0468, -0.0631,  0.0006,  0.0492,  0.0757,\n",
      "          0.0879,  0.0823],\n",
      "        [-0.0766,  0.0066,  0.0799,  0.0427, -0.0577,  0.0006,  0.0449,  0.0691,\n",
      "          0.0803,  0.0751],\n",
      "        [-0.0726,  0.0062,  0.0758,  0.0405, -0.0546,  0.0006,  0.0426,  0.0655,\n",
      "          0.0761,  0.0712],\n",
      "        [-0.0804,  0.0069,  0.0839,  0.0448, -0.0605,  0.0006,  0.0472,  0.0725,\n",
      "          0.0842,  0.0788],\n",
      "        [-0.0273,  0.0023,  0.0285,  0.0152, -0.0206,  0.0002,  0.0160,  0.0246,\n",
      "          0.0286,  0.0268],\n",
      "        [-0.0480,  0.0041,  0.0501,  0.0268, -0.0361,  0.0004,  0.0281,  0.0433,\n",
      "          0.0503,  0.0471],\n",
      "        [-0.0759,  0.0065,  0.0791,  0.0423, -0.0571,  0.0006,  0.0445,  0.0684,\n",
      "          0.0794,  0.0744],\n",
      "        [-0.0426,  0.0036,  0.0445,  0.0238, -0.0321,  0.0003,  0.0250,  0.0384,\n",
      "          0.0446,  0.0418],\n",
      "        [-0.1019,  0.0087,  0.1062,  0.0568, -0.0766,  0.0008,  0.0597,  0.0918,\n",
      "          0.1067,  0.0998]])\n",
      "Second hidden layer bias gradient: \n",
      " tensor([-0.1329,  0.0114,  0.1386,  0.0740, -0.0999,  0.0010,  0.0779,  0.1198,\n",
      "         0.1391,  0.1302])\n",
      "Output layer bias gradient: \n",
      " tensor([-1.9866])\n",
      "Output layer weight gradient: \n",
      " tensor([[-0.6655],\n",
      "        [-0.9934],\n",
      "        [-1.1257],\n",
      "        [-1.0672],\n",
      "        [-1.2474],\n",
      "        [-0.9371],\n",
      "        [-1.0377],\n",
      "        [-0.9244],\n",
      "        [-1.2230],\n",
      "        [-0.8619]])\n",
      "Predicted y: tensor([[-0.5751]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_printoptions(precision=4)\n",
    "#torch.__version__ 1.7.1\n",
    "import numpy as np\n",
    "\n",
    "N, D_in, H1, H2, D_out = 1, 2, 10, 10, 1\n",
    "\n",
    "'''N is batch size, set N as 1 is what Q5 imply'''\n",
    "\n",
    "torch.manual_seed(0)\n",
    "X = torch.rand(N,2)\n",
    "y = torch.empty(N,1)\n",
    "y[0,0] = (X[0][0]**2 + X[0][1]**2)/2\n",
    "print(\"Input X: \",X)\n",
    "print(\"Input y: \",y)\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H1,H2, D_out):\n",
    "        \n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "        self.linear2 = torch.nn.Linear(H1, H2)\n",
    "        self.linear3 = torch.nn.Linear(H2, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigmoid = torch.nn.Sigmoid()        \n",
    "        h1_sigmoid = sigmoid(self.linear1(x))       \n",
    "        h2_sigmoid = sigmoid(self.linear2(h1_sigmoid))  \n",
    "        y_pred = self.linear3(h2_sigmoid)\n",
    "        return y_pred\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H1,H2, D_out)\n",
    "\n",
    "#Save the initial parameters for my own defined BP process\n",
    "b3 = model.linear3.bias\n",
    "w3 = model.linear3.weight.t()\n",
    "b2 = model.linear2.bias\n",
    "w2 = model.linear2.weight.t()\n",
    "b1 = model.linear1.bias\n",
    "w1 = model.linear1.weight.t()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "#def loss_fn(y, y_pred):\n",
    "#    return torch.sum((y_pred - y) ** 2) #pretty the same with MSELoss\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "for t in range(1):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print('step %d: loss=%f   ' % (t+1, loss.item()))   \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #As we only need the 1st time gradient, no need to update the parameters\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    '''  \n",
    "    \n",
    "    # in Linear class __init__ï¼Œthe shape of weight[out_features, in_features], need to be transformed\n",
    "    print(\"First hidden layer weights gradient: \\n\",model.linear1.weight.grad.t())\n",
    "    print(\"First hidden layer bias gradient: \\n\", model.linear1.bias.grad)\n",
    "    print(\"Second hidden layer weight gradient:\\n\", model.linear2.weight.grad.t())\n",
    "    print(\"Second hidden layer bias gradient: \\n\",model.linear2.bias.grad)\n",
    "    print(\"Output layer bias gradient: \\n\",model.linear3.bias.grad)\n",
    "    print(\"Output layer weight gradient: \\n\",model.linear3.weight.grad.t())\n",
    "    \n",
    "print(\"Predicted y:\",y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(7) Implement the forward propagation and backpropagation algorithm from scratch,without using pytorch autograd, compute the gradients using your implementation\n",
    "\n",
    "a. dL/dw, dL/db\n",
    "\n",
    "b. Print these values into a text file: my_autograd.dat\n",
    "\n",
    "(8) Compare the two files torch_autograd.dat and my_autograd.dat and show that they give the same values up to numerical precision errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=0.986679   \n",
      "First hidden layer weights gradient: \n",
      " tensor([[-0.0028,  0.0071, -0.0034, -0.0120, -0.0009, -0.0063, -0.0078, -0.0040,\n",
      "          0.0051,  0.0050],\n",
      "        [-0.0043,  0.0110, -0.0053, -0.0185, -0.0015, -0.0097, -0.0121, -0.0062,\n",
      "          0.0080,  0.0078]], grad_fn=<MmBackward>)\n",
      "First hidden layer bias gradient: \n",
      " tensor([[-0.0056,  0.0143, -0.0069, -0.0241, -0.0019, -0.0126, -0.0158, -0.0081,\n",
      "          0.0104,  0.0102]], grad_fn=<MulBackward0>)\n",
      "Second hidden layer weight gradient:\n",
      " tensor([[-0.0402,  0.0034,  0.0420,  0.0224, -0.0303,  0.0003,  0.0236,  0.0363,\n",
      "          0.0421,  0.0394],\n",
      "        [-0.0839,  0.0072,  0.0875,  0.0468, -0.0631,  0.0006,  0.0492,  0.0757,\n",
      "          0.0879,  0.0823],\n",
      "        [-0.0766,  0.0066,  0.0799,  0.0427, -0.0577,  0.0006,  0.0449,  0.0691,\n",
      "          0.0803,  0.0751],\n",
      "        [-0.0726,  0.0062,  0.0758,  0.0405, -0.0546,  0.0006,  0.0426,  0.0655,\n",
      "          0.0761,  0.0712],\n",
      "        [-0.0804,  0.0069,  0.0839,  0.0448, -0.0605,  0.0006,  0.0472,  0.0725,\n",
      "          0.0842,  0.0788],\n",
      "        [-0.0273,  0.0023,  0.0285,  0.0152, -0.0206,  0.0002,  0.0160,  0.0246,\n",
      "          0.0286,  0.0268],\n",
      "        [-0.0480,  0.0041,  0.0501,  0.0268, -0.0361,  0.0004,  0.0281,  0.0433,\n",
      "          0.0503,  0.0471],\n",
      "        [-0.0759,  0.0065,  0.0791,  0.0423, -0.0571,  0.0006,  0.0445,  0.0684,\n",
      "          0.0794,  0.0744],\n",
      "        [-0.0426,  0.0036,  0.0445,  0.0238, -0.0321,  0.0003,  0.0250,  0.0384,\n",
      "          0.0446,  0.0418],\n",
      "        [-0.1019,  0.0087,  0.1062,  0.0568, -0.0766,  0.0008,  0.0597,  0.0918,\n",
      "          0.1067,  0.0998]], grad_fn=<MmBackward>)\n",
      "Second hidden layer bias gradient: \n",
      " tensor([[-0.1329,  0.0114,  0.1386,  0.0740, -0.0999,  0.0010,  0.0779,  0.1198,\n",
      "          0.1391,  0.1302]], grad_fn=<MulBackward0>)\n",
      "Output layer bias gradient: \n",
      " tensor([[-1.9866]], grad_fn=<MulBackward0>)\n",
      "Output layer weight gradient: \n",
      " tensor([[-0.6655],\n",
      "        [-0.9934],\n",
      "        [-1.1257],\n",
      "        [-1.0672],\n",
      "        [-1.2474],\n",
      "        [-0.9371],\n",
      "        [-1.0377],\n",
      "        [-0.9244],\n",
      "        [-1.2230],\n",
      "        [-0.8619]], grad_fn=<MmBackward>)\n",
      "tensor([[-0.5751]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_derivationx(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "learning_rate = 0.1\n",
    "for t in range(1):\n",
    "    # Forward pass: compute predicted y \n",
    "    h1 = X.mm(w1) +b1 # N*D_in x D_in* H1 + N*H1 = N*H1\n",
    "    h1_sigmoid = torch.sigmoid(h1)   #N*H1\n",
    "    h2 = h1_sigmoid.mm(w2) +b2   #N*H1 x H1*H2 + N*H2  = N *H2\n",
    "    h2_sigmoid = torch.sigmoid(h2)#N * H2   \n",
    "    y_pred  = h2_sigmoid.mm(w3) +b3 #N*H2 x H2*D_out + N*D_out= N * D_out\n",
    "         \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print('step %d: loss=%f   ' % (t+1, loss))\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = -2.0 *(y - y_pred)#N    \n",
    "    delta = grad_y_pred#N*D_out                           \n",
    "    grad_w3 = h2_sigmoid.T.mm(delta)#H2*N mm N*D_out =    H2* D_out                    \n",
    "    grad_b3 = delta  #N*D_out \n",
    "    \n",
    "    delta = delta.mm(w3.T)*sigmoid_derivationx(h2_sigmoid)#N*D_out x D_out*H2 = N*10 . N *10 =N *10\n",
    "    grad_w2 = h1_sigmoid.T.mm(delta)#10*N  x N*10 = 10*10\n",
    "    grad_b2 = delta   # 1*N x N*10  = 1*10\n",
    "    \n",
    "    delta = delta.mm(w2.T)*sigmoid_derivationx(h1_sigmoid)#N*10 X 10*10 = N *10 * N *10 = N *10\n",
    "    grad_w1 = X.T.mm(delta)# 2*N X N*10 = 2*10\n",
    "    grad_b1 =  delta  # 1*N x N * 10 \n",
    "     \n",
    "    print(\"First hidden layer weights gradient: \\n\",grad_w1)\n",
    "    print(\"First hidden layer bias gradient: \\n\", grad_b1)\n",
    "    print(\"Second hidden layer weight gradient:\\n\", grad_w2)\n",
    "    print(\"Second hidden layer bias gradient: \\n\",grad_b2)\n",
    "    print(\"Output layer bias gradient: \\n\",grad_b3)   \n",
    "    print(\"Output layer weight gradient: \\n\",grad_w3)  \n",
    "    \n",
    "    #update weights\n",
    "    w3 = w3 - learning_rate * grad_w3  \n",
    "    w2 = w2 - learning_rate * grad_w2\n",
    "    w1 = w1 - learning_rate * grad_w1\n",
    "    \n",
    "    b3 = b3 - learning_rate * grad_b3\n",
    "    b2 = b2 - learning_rate * grad_b2\n",
    "    b1 = b1 - learning_rate * grad_b1  \n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
