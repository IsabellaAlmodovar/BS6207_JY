{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1: due week 5 just before class at 1759 hours\n",
    "Given a fully connected Neural Network as follows:\n",
    "1. Input (x1,x2): 2 nodes\n",
    "\n",
    "2. First hidden layer: 10 nodes, with weights (w) and bias (b), sigmoid activation function\n",
    "\n",
    "3. Second hidden layer: 10 nodes, with weights (w) and bias (b), sigmoid activation function\n",
    "\n",
    "4. Output (predict): 1 node\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Implement this neural network in pytorch\n",
    "\n",
    "(2) Generate the input data (x1,x2) \\in [0,1] drawn from a uniform random distribution\n",
    "\n",
    "(3) Generate the labels y = (x1*x1+x2*x2)/2\n",
    "\n",
    "(4) Implement a loss function L = (predict-y)^2\n",
    "\n",
    "(5) Use batch size of 1, that means feed data one point at a time into network and compute the loss. Do one time forward propagation with one data point.\n",
    "\n",
    "(6) Compute the gradients using pytorch autograd:\n",
    "\n",
    "a. dL/dw, dL/db\n",
    "\n",
    "b. Print these values into a text file: torch_autograd.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X:  tensor([[0.4962566, 0.7682218]])\n",
      "Input y:  tensor([[0.4182177]])\n",
      "step 1: loss=0.986679   \n",
      "First hidden layer weights gradient: \n",
      " tensor([[-0.0027822,  0.0071077, -0.0034240, -0.0119751, -0.0009449, -0.0062731,\n",
      "         -0.0078180, -0.0039982,  0.0051486,  0.0050384],\n",
      "        [-0.0043070,  0.0110030, -0.0053005, -0.0185379, -0.0014627, -0.0097110,\n",
      "         -0.0121026, -0.0061894,  0.0079701,  0.0077997]])\n",
      "First hidden layer bias gradient: \n",
      " tensor([-0.0056064,  0.0143226, -0.0068997, -0.0241309, -0.0019041, -0.0126409,\n",
      "        -0.0157540, -0.0080567,  0.0103748,  0.0101529])\n",
      "Second hidden layer weight gradient:\n",
      " tensor([[-0.0402310,  0.0034391,  0.0419600,  0.0224192, -0.0302611,  0.0003073,\n",
      "          0.0235890,  0.0362652,  0.0421244,  0.0394331],\n",
      "        [-0.0839402,  0.0071756,  0.0875477,  0.0467767, -0.0631384,  0.0006411,\n",
      "          0.0492174,  0.0756657,  0.0878907,  0.0822755],\n",
      "        [-0.0766456,  0.0065520,  0.0799396,  0.0427117, -0.0576515,  0.0005854,\n",
      "          0.0449403,  0.0690902,  0.0802528,  0.0751255],\n",
      "        [-0.0726444,  0.0062100,  0.0757664,  0.0404820, -0.0546418,  0.0005548,\n",
      "          0.0425942,  0.0654834,  0.0760632,  0.0712036],\n",
      "        [-0.0804233,  0.0068750,  0.0838796,  0.0448169, -0.0604930,  0.0006143,\n",
      "          0.0471553,  0.0724955,  0.0842082,  0.0788283],\n",
      "        [-0.0273338,  0.0023366,  0.0285085,  0.0152321, -0.0205600,  0.0002088,\n",
      "          0.0160269,  0.0246393,  0.0286202,  0.0267917],\n",
      "        [-0.0480085,  0.0041040,  0.0500718,  0.0267534, -0.0361112,  0.0003667,\n",
      "          0.0281493,  0.0432761,  0.0502680,  0.0470564],\n",
      "        [-0.0758615,  0.0064850,  0.0791218,  0.0422748, -0.0570617,  0.0005794,\n",
      "          0.0444806,  0.0683834,  0.0794318,  0.0743570],\n",
      "        [-0.0426264,  0.0036439,  0.0444584,  0.0237541, -0.0320629,  0.0003256,\n",
      "          0.0249936,  0.0384245,  0.0446326,  0.0417811],\n",
      "        [-0.1018669,  0.0087081,  0.1062449,  0.0567666, -0.0766225,  0.0007780,\n",
      "          0.0597286,  0.0918253,  0.1066611,  0.0998467]])\n",
      "Second hidden layer bias gradient: \n",
      " tensor([-0.1328733,  0.0113586,  0.1385838,  0.0740453, -0.0999450,  0.0010149,\n",
      "         0.0779088,  0.1197752,  0.1391267,  0.1302381])\n",
      "Output layer bias gradient: \n",
      " tensor([-1.9866347])\n",
      "Output layer weight gradient: \n",
      " tensor([[-0.6655160],\n",
      "        [-0.9933792],\n",
      "        [-1.1256515],\n",
      "        [-1.0672039],\n",
      "        [-1.2473825],\n",
      "        [-0.9371449],\n",
      "        [-1.0377069],\n",
      "        [-0.9243824],\n",
      "        [-1.2229568],\n",
      "        [-0.8619202]])\n",
      "Predicted y: tensor([[-0.5750997]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_printoptions(precision=7)\n",
    "#torch.__version__ 1.7.1\n",
    "import numpy as np\n",
    "\n",
    "N, D_in, H1, H2, D_out = 1, 2, 10, 10, 1\n",
    "\n",
    "'''N is batch size, set N as 1 is what Q5 imply'''\n",
    "\n",
    "torch.manual_seed(0)\n",
    "X = torch.rand(N,2)\n",
    "y = torch.empty(N,1)\n",
    "y[0,0] = (X[0][0]**2 + X[0][1]**2)/2\n",
    "print(\"Input X: \",X)\n",
    "print(\"Input y: \",y)\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H1,H2, D_out):\n",
    "        \n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "        self.linear2 = torch.nn.Linear(H1, H2)\n",
    "        self.linear3 = torch.nn.Linear(H2, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigmoid = torch.nn.Sigmoid()        \n",
    "        h1_sigmoid = sigmoid(self.linear1(x))       \n",
    "        h2_sigmoid = sigmoid(self.linear2(h1_sigmoid))  \n",
    "        y_pred = self.linear3(h2_sigmoid)\n",
    "        return y_pred\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H1,H2, D_out)\n",
    "\n",
    "#Save the initial parameters for my own defined BP process\n",
    "b3 = model.linear3.bias\n",
    "w3 = model.linear3.weight.t()\n",
    "b2 = model.linear2.bias\n",
    "w2 = model.linear2.weight.t()\n",
    "b1 = model.linear1.bias\n",
    "w1 = model.linear1.weight.t()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "#def loss_fn(y, y_pred):\n",
    "#    return torch.sum((y_pred - y) ** 2) #pretty the same with MSELoss\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "for t in range(1):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print('step %d: loss=%f   ' % (t+1, loss.item()))   \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #As we only need the 1st time gradient, no need to update the parameters\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    '''  \n",
    "    \n",
    "    # in Linear class __init__ï¼Œthe shape of weight[out_features, in_features], need to be transformed\n",
    "    print(\"First hidden layer weights gradient: \\n\",model.linear1.weight.grad.t())\n",
    "    print(\"First hidden layer bias gradient: \\n\", model.linear1.bias.grad)\n",
    "    print(\"Second hidden layer weight gradient:\\n\", model.linear2.weight.grad.t())\n",
    "    print(\"Second hidden layer bias gradient: \\n\",model.linear2.bias.grad)\n",
    "    print(\"Output layer bias gradient: \\n\",model.linear3.bias.grad)\n",
    "    print(\"Output layer weight gradient: \\n\",model.linear3.weight.grad.t())\n",
    "    \n",
    "print(\"Predicted y:\",y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(7) Implement the forward propagation and backpropagation algorithm from scratch,without using pytorch autograd, compute the gradients using your implementation\n",
    "\n",
    "a. dL/dw, dL/db\n",
    "\n",
    "b. Print these values into a text file: my_autograd.dat\n",
    "\n",
    "(8) Compare the two files torch_autograd.dat and my_autograd.dat and show that they give the same values up to numerical precision errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=0.986679   \n",
      "First hidden layer weights gradient: \n",
      " tensor([[-0.0027822,  0.0071077, -0.0034240, -0.0119751, -0.0009449, -0.0062731,\n",
      "         -0.0078180, -0.0039982,  0.0051486,  0.0050384],\n",
      "        [-0.0043070,  0.0110030, -0.0053005, -0.0185379, -0.0014627, -0.0097110,\n",
      "         -0.0121026, -0.0061894,  0.0079701,  0.0077997]],\n",
      "       grad_fn=<MmBackward>)\n",
      "First hidden layer bias gradient: \n",
      " tensor([[-0.0056064,  0.0143226, -0.0068997, -0.0241309, -0.0019041, -0.0126409,\n",
      "         -0.0157540, -0.0080567,  0.0103748,  0.0101529]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Second hidden layer weight gradient:\n",
      " tensor([[-0.0402310,  0.0034391,  0.0419600,  0.0224192, -0.0302611,  0.0003073,\n",
      "          0.0235890,  0.0362652,  0.0421244,  0.0394331],\n",
      "        [-0.0839402,  0.0071756,  0.0875477,  0.0467767, -0.0631384,  0.0006411,\n",
      "          0.0492174,  0.0756657,  0.0878907,  0.0822755],\n",
      "        [-0.0766456,  0.0065520,  0.0799396,  0.0427117, -0.0576515,  0.0005854,\n",
      "          0.0449403,  0.0690902,  0.0802528,  0.0751255],\n",
      "        [-0.0726444,  0.0062100,  0.0757664,  0.0404820, -0.0546418,  0.0005548,\n",
      "          0.0425942,  0.0654834,  0.0760632,  0.0712036],\n",
      "        [-0.0804233,  0.0068750,  0.0838796,  0.0448169, -0.0604930,  0.0006143,\n",
      "          0.0471553,  0.0724955,  0.0842082,  0.0788283],\n",
      "        [-0.0273338,  0.0023366,  0.0285085,  0.0152321, -0.0205600,  0.0002088,\n",
      "          0.0160269,  0.0246393,  0.0286202,  0.0267917],\n",
      "        [-0.0480085,  0.0041040,  0.0500718,  0.0267534, -0.0361112,  0.0003667,\n",
      "          0.0281493,  0.0432761,  0.0502680,  0.0470564],\n",
      "        [-0.0758615,  0.0064850,  0.0791218,  0.0422748, -0.0570617,  0.0005794,\n",
      "          0.0444806,  0.0683834,  0.0794318,  0.0743570],\n",
      "        [-0.0426264,  0.0036439,  0.0444584,  0.0237541, -0.0320629,  0.0003256,\n",
      "          0.0249936,  0.0384245,  0.0446326,  0.0417811],\n",
      "        [-0.1018669,  0.0087081,  0.1062449,  0.0567666, -0.0766225,  0.0007780,\n",
      "          0.0597286,  0.0918253,  0.1066611,  0.0998467]],\n",
      "       grad_fn=<MmBackward>)\n",
      "Second hidden layer bias gradient: \n",
      " tensor([[-0.1328733,  0.0113586,  0.1385838,  0.0740453, -0.0999450,  0.0010149,\n",
      "          0.0779088,  0.1197752,  0.1391267,  0.1302381]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Output layer bias gradient: \n",
      " tensor([[-1.9866347]], grad_fn=<MulBackward0>)\n",
      "Output layer weight gradient: \n",
      " tensor([[-0.6655160],\n",
      "        [-0.9933792],\n",
      "        [-1.1256515],\n",
      "        [-1.0672039],\n",
      "        [-1.2473825],\n",
      "        [-0.9371449],\n",
      "        [-1.0377069],\n",
      "        [-0.9243824],\n",
      "        [-1.2229568],\n",
      "        [-0.8619202]], grad_fn=<MmBackward>)\n",
      "tensor([[-0.5750997]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_derivationx(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "learning_rate = 0.1\n",
    "for t in range(1):\n",
    "    # Forward pass: compute predicted y \n",
    "    h1 = X.mm(w1) +b1 # N*D_in x D_in* H1 + N*H1 = N*H1\n",
    "    h1_sigmoid = torch.sigmoid(h1)   #N*H1\n",
    "    h2 = h1_sigmoid.mm(w2) +b2   #N*H1 x H1*H2 + N*H2  = N *H2\n",
    "    h2_sigmoid = torch.sigmoid(h2)#N * H2   \n",
    "    y_pred  = h2_sigmoid.mm(w3) +b3 #N*H2 x H2*D_out + N*D_out= N * D_out\n",
    "         \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print('step %d: loss=%f   ' % (t+1, loss))\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = -2.0 *(y - y_pred)#N    \n",
    "    delta = grad_y_pred#N*D_out                           \n",
    "    grad_w3 = h2_sigmoid.T.mm(delta)#H2*N mm N*D_out =    H2* D_out                    \n",
    "    grad_b3 = delta  #N*D_out \n",
    "    \n",
    "    delta = delta.mm(w3.T)*sigmoid_derivationx(h2_sigmoid)#N*D_out x D_out*H2 = N*10 . N *10 =N *10\n",
    "    grad_w2 = h1_sigmoid.T.mm(delta)#10*N  x N*10 = 10*10\n",
    "    grad_b2 = delta   # 1*N x N*10  = 1*10\n",
    "    \n",
    "    delta = delta.mm(w2.T)*sigmoid_derivationx(h1_sigmoid)#N*10 X 10*10 = N *10 * N *10 = N *10\n",
    "    grad_w1 = X.T.mm(delta)# 2*N X N*10 = 2*10\n",
    "    grad_b1 =  delta  # 1*N x N * 10 \n",
    "     \n",
    "    print(\"First hidden layer weights gradient: \\n\",grad_w1)\n",
    "    print(\"First hidden layer bias gradient: \\n\", grad_b1)\n",
    "    print(\"Second hidden layer weight gradient:\\n\", grad_w2)\n",
    "    print(\"Second hidden layer bias gradient: \\n\",grad_b2)\n",
    "    print(\"Output layer bias gradient: \\n\",grad_b3)   \n",
    "    print(\"Output layer weight gradient: \\n\",grad_w3)  \n",
    "    \n",
    "    #update weights\n",
    "    w3 = w3 - learning_rate * grad_w3  \n",
    "    w2 = w2 - learning_rate * grad_w2\n",
    "    w1 = w1 - learning_rate * grad_w1\n",
    "    \n",
    "    b3 = b3 - learning_rate * grad_b3\n",
    "    b2 = b2 - learning_rate * grad_b2\n",
    "    b1 = b1 - learning_rate * grad_b1  \n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss=0.986679   \n",
      "[[-0.665516  ]\n",
      " [-0.9933792 ]\n",
      " [-1.1256515 ]\n",
      " [-1.0672039 ]\n",
      " [-1.2473825 ]\n",
      " [-0.93714494]\n",
      " [-1.0377069 ]\n",
      " [-0.9243824 ]\n",
      " [-1.2229568 ]\n",
      " [-0.86192024]]\n",
      "[[0.41821766]] [[-0.5750997]]\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H1, H2, D_out = 1, 2, 10, 10, 1 #Only N: number of input can be tuned\n",
    "np.random.seed(10)\n",
    "\n",
    "X = X.numpy()\n",
    "y = y.numpy()\n",
    "w1 = w1.detach().numpy()\n",
    "w2 = w2.detach().numpy()\n",
    "w3 = w3.detach().numpy()\n",
    "\n",
    "b1 = b1.detach().numpy()\n",
    "b2 = b2.detach().numpy()\n",
    "b3 = b3.detach().numpy()\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "def sigmoid(x):\n",
    "        return 1.0/(1+np.exp(-x))\n",
    "    \n",
    "def sigmoid_derivationx(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "for t in range(1):\n",
    "    # forward\n",
    "    h1 = X.dot(w1) +b1 # N*2 x 2* 10 + 1*10 = N*10\n",
    "    h1_sigmoid = sigmoid(h1)#N*10\n",
    "    \n",
    "    h2 = h1_sigmoid.dot(w2) +b2 #N*10 x 10*10 + 1*10  = N *10\n",
    "    h2_sigmoid = sigmoid(h2)#N * 10\n",
    "    \n",
    "    h3 = h2_sigmoid.dot(w3) +b3 #N*10 x 10*1 = N * 1\n",
    "    y_pred = h3\n",
    "    #print(y_pred)\n",
    "    #compute loss L = (predict-y)^2\n",
    "    loss = np.square(y_pred -y).sum() #N \n",
    "    print('step %d: loss=%f   ' % (t, loss))    \n",
    "    #backward pass\n",
    "    #compute gradient\n",
    "    grad_y_pred = 2.0 *(y_pred - y)#N    \n",
    "    \n",
    "    delta = grad_y_pred#N*1\n",
    "    grad_w3 = np.dot(h2_sigmoid.T, delta)#10*N, N*1 = 10*1\n",
    "    grad_b3 = np.ones(N).dot(delta)   # 1*N(h3 shape.T) x N*1 =1\n",
    "    \n",
    "    \n",
    "    delta = np.dot(delta, w3.T)*sigmoid_derivationx(h2_sigmoid)#N*1 x 1*10 = N*10 . N *10 =N *10\n",
    "    grad_w2 = np.dot(h1_sigmoid.T , delta)#10*N  x N*10 = 10*10\n",
    "    grad_b2 = np.ones(N).dot(delta)   # 1*N x N*10  = 1*10\n",
    "    \n",
    "    delta = np.dot(delta, w2.T)*sigmoid_derivationx(h1_sigmoid)#N*10 X 10*10 = N *10 * N *10 = N *10\n",
    "    grad_w1 = np.dot(X.T , delta)# 2*N X N*10 = 2*10\n",
    "    grad_b1 =  np.ones(N).dot(delta)  # 1*N x N * 10 \n",
    "    print(grad_w3)\n",
    "    #update weights\n",
    "    w3 -= learning_rate * grad_w3\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    \n",
    "    b3 -= learning_rate * grad_b3\n",
    "    b2 -= learning_rate * grad_b2\n",
    "    b1 -= learning_rate * grad_b1\n",
    "    \n",
    "print(y, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
